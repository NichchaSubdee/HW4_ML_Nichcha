---
title: "**Assignment 4**"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

#### **Topic:** Physical Activity and Transit (PAT) Survey Wave 1/ Wave 2

**Set up needed libraries**

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(cluster)
library(factoextra)

set.seed(123)
```

**Import dataset**
```{r import, echo =FALSE, message=FALSE}
p1_df = read_csv(file = "./class4_p1.csv")
p1_df
```

**Description:** Based on the initial inspection of the dataset, there are 17 columns and 3,811 rows. All variables were imported as double numeric (dbl) values. However, according to the codebook, several variables represent categorical survey responses rather than continuous measurements. Therefore, variables were recoded as either categorical (factor) or continuous (numeric) to ensure they are stored and analyzed appropriately.

### **Part I: Implementing a Simple Prediction Pipeline**

The New York City Department of Health administered a questionnaire on general health and physical activity among residents. Using the dataset `class4_p1.csv`, fit and evaluate **two prediction models** using linear regression. 

The aim of the models are to predict _the number of days in a month_ an individual reported having good physical health (feature name: _healthydays_). 

Your analytic pipeline should include the following:

**1. Perform basic data cleaning.** 

```{r Q1, message=FALSE}
p1_df_clean = p1_df %>%
  # Drop the first column
  select(-`...1`) %>%
  
  # Convert categorical variables to factors
  mutate(
    chronic1 = as.factor(chronic1),
    chronic3 = as.factor(chronic3),
    chronic4 = as.factor(chronic4),
    tobacco1 = as.factor(tobacco1),
    alcohol1 = as.factor(alcohol1),
    habits5 = as.factor(habits5),
    habits7 = as.factor(habits7),
    agegroup = as.factor(agegroup),
    dem3 = as.factor(dem3),
    dem4 = as.factor(dem4),
    dem8 = as.factor(dem8),
    povertygroup = as.factor(povertygroup)
  ) %>%
  
  # Ensure continuous variables are numeric
  mutate(
    bmi = as.numeric(bmi),
    gpaq8totmin = as.numeric(gpaq8totmin),
    gpaq11days = as.numeric(gpaq11days),
    healthydays = as.numeric(healthydays)
  )

p1_df_clean %>%
  glimpse()
```


**2. Partition data into training and testing (use a 70/30 split)**

```{r Q2, message=FALSE}
# Remove rows with missing (NA) outcome variable
p1_df_clean = p1_df_clean %>%
  filter(!is.na(healthydays))

# Create training and testing partitions
train_index = createDataPartition(p1_df_clean$healthydays, 
                                  p = 0.70, list = FALSE)

train_df = p1_df_clean[train_index, ]
test_df  = p1_df_clean[-train_index, ]

# Check number of observation in each set
cat("Training set:", nrow(train_df), "observations\n")
cat("Test set:", nrow(test_df), "observations\n")
```

3. Fit two prediction models using different subsets of the features in the training data. Features can overlap in the two models, but the feature sets should not be exactly the same across models. Clearly state which features were used in the two models.

```{r Q3_0, message=FALSE}
# Set up cross-validation
control_settings = trainControl(method = "cv", number = 10)
```


**Model 1: Demographic characteristics (age, sex, poverty) and chronic disease predictors (hypertension, diabetes, asthma) ot predict healthy days**

Features: `agegroup`,`dem3`,`povertygroup`,`chronic1`,`chronic3`,`chronic4`,`bmi`

```{r Q3_1, message=FALSE}
set.seed(123)

model1 <- train(
  healthydays ~ agegroup + dem3 + povertygroup + chronic1 + chronic3 + chronic4 + bmi,
  data = train_df,
  method = "lm",
  preProc = c("center", "scale"),
  trControl = control_settings,
  metric = "RMSE",
  na.action = na.omit
)

# Model 1 results
model1
model1$results
summary(model1$finalModel)

```

**Model 2: Adding more predictors into Model 1 by adding physical activity variables (days walked for transportation, minutes of home physical activity, self-rated activity level) and lifestyle factors (smoking, alcohol assumption)**

Features: Model 1 and `gpaq8totmin`, `gpaq11days`, `habits5`, `tobacco1`, `alcohol1`

```{r Q3_2, message=FALSE}
set.seed(123)

model2 <- train(
  healthydays ~ agegroup + dem3 + povertygroup + chronic1 + chronic3 + chronic4 + 
                bmi + gpaq8totmin + gpaq11days + habits5 + tobacco1 + alcohol1,
  data = train_df,
  method = "lm",
  preProc = c("center", "scale"),
  trControl = control_settings,
  metric = "RMSE",
  na.action = na.omit
)

# Model 2 results
model2
model2$results
summary(model2$finalModel)
```


4. Apply both models within the test data and determine which model is the preferred prediction model using the appropriate evaluation metric(s). 

```{r Q4_1, message=FALSE}
# Generate predictions on test set
pred1 <- predict(model1, newdata = test_df)
pred2 <- predict(model2, newdata = test_df)

# Identify complete cases (rows without missing predictions)
complete_cases1 = complete.cases(test_df[, c("agegroup", "dem3", "povertygroup", "chronic1", "chronic3", "chronic4", "bmi")])
complete_cases2 = complete.cases(test_df[, c("agegroup", "dem3", "povertygroup", "chronic1", "chronic3", "chronic4", "bmi",            "gpaq8totmin", "gpaq11days", "habits5", "tobacco1", "alcohol1")])

# Create results data frames with matching rows
test_outcome_model1 = data.frame(
  observed = test_df$healthydays[complete_cases1],
  predicted = pred1
)

test_outcome_model2 = data.frame(
  observed = test_df$healthydays[complete_cases2],
  predicted = pred2
)

# Calculate evaluation metrics
eval_model1 = postResample(pred = test_outcome_model1$predicted, 
                            obs = test_outcome_model1$observed)
eval_model2 = postResample(pred = test_outcome_model2$predicted, 
                            obs = test_outcome_model2$observed)

# Create comparison table
model_comparison = data.frame(
  Model = c("Model 1: Demographics + Health", 
            "Model 2: Demographics + Health + PA + Lifestyle"),
  RMSE = c(eval_model1["RMSE"], eval_model2["RMSE"]),
  Rsquared = c(eval_model1["Rsquared"], eval_model2["Rsquared"]),
  MAE = c(eval_model1["MAE"], eval_model2["MAE"])
)

model_comparison %>%
  print()

# Determine preferred model
cat("\n**Model Performance Summary:**\n")
cat("Model 1 RMSE:", round(eval_model1["RMSE"], 3), "\n")
cat("Model 2 RMSE:", round(eval_model2["RMSE"], 3), "\n\n")

if (eval_model2["RMSE"] < eval_model1["RMSE"]) {
  cat("**Preferred Model: Model 2**\n")
  cat("Model 2 achieves lower prediction error (RMSE =", round(eval_model2["RMSE"], 3),") compared to Model 1 (RMSE =", round(eval_model1["RMSE"], 3), ").\n")
  cat("Model 2 explains", round(eval_model2["Rsquared"] * 100, 1), 
      "% of variance compared to", round(eval_model1["Rsquared"] * 100, 1), "% for Model 1.\n")
} else {
  cat("**Preferred Model: Model 1**\n")
  cat("Model 1 achieves lower prediction error despite using fewer features.\n")
}
```

```{r Q4_2, message =FALSE}
# Visualize predictions vs observed values
par(mfrow = c(1, 2))

# Model 1
plot(test_outcome_model1$observed, test_outcome_model1$predicted,
     xlab = "Observed Healthy Days",
     ylab = "Predicted Healthy Days",
     main = "Model 1: Predicted vs Observed",
     pch = 16, col = rgb(0, 0, 1, 0.5))
abline(0, 1, col = "red", lwd = 2)

# Model 2
plot(test_outcome_model2$observed, test_outcome_model2$predicted,
     xlab = "Observed Healthy Days",
     ylab = "Predicted Healthy Days",
     main = "Model 2: Predicted vs Observed",
     pch = 16, col = rgb(0, 0.5, 0, 0.5))
abline(0, 1, col = "red", lwd = 2)
```

5. Describe one setting (in 1-2 sentences) where the implementation of your final model would be useful.

**Answer:** The final prediction model could be implemented in New York City's public health surveillance system to identify residents at risk of poor physical health days. By collecting basic demographic, health status, and physical activity information through routine community health surveys, public health officials can predict which individuals are likely to experience fewer healthy days per month and prioritize them for targeted interventions such as chronic disease management programs, community exercise initiatives, or health coaching services.


### **Part II: Conducting an Unsupervised Analysis**

Using the dataset from the Group assignment Part 3 (USArrests), identify clusters using hierarchical analysis.

6. Conduct a hierarchical clustering analysis. Use a Euclidian distance measure to construct your dissimilarity matrix. Use complete linkage.

7. Determine the optimal number of clusters using a clear, data-driven strategy.

8. Describe the composition of each cluster in terms of the original input features

9. Pretend that the data are from 2022 and not 1973. Describe one research question that can be addressed using the newly identified clusters. Briefly comment on any scientific or ethical considerations one should review before using these clusters for your specific question. NOTE: The clusters can be used as an exposure, an outcome or a covariate. (2-4 sentences)

10. Optional Repeat analysis with a different linkage method (e.g. single or average). Do the clusters change?

### **AI tools**

Claude.ai was used in this assignment to assist with debugging R code syntax errors and editing text for clarity and conciseness.
